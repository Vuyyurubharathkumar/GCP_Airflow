GCP Data Pipeline Automation with Apache Airflow, Cloud Data Fusion & Looker
Overview
This project automates the extraction, processing, and visualization of employee data using Google Cloud Platform (GCP) services such as Cloud Composer (Apache Airflow), Cloud Data Fusion, BigQuery, Cloud Storage, and Looker. The pipeline is orchestrated with Apache Airflow, ensuring end-to-end automation from data ingestion to visualization.

Project Workflow
1. Cloud Composer Setup (Apache Airflow):
Created a Cloud Composer environment for managing the ETL pipeline.

2. Cloud Data Fusion Instance:
Set up an instance of Cloud Data Fusion for data ingestion and transformation.

3. Automated Data Extraction & Upload:
Developed a Python script to extract employee data and save it as a CSV file.

Uploaded the CSV file automatically to Google Cloud Storage (GCS).

4. Data Processing with Cloud Data Fusion:
Used Wrangler in Cloud Data Fusion for data cleansing and transformation.

Designed a Batch Pipeline that processes the data.

Connected the pipeline to BigQuery (Sink) and deployed it.

5. Apache Airflow DAG for Automation:
Created an Airflow DAG to orchestrate the data workflow.

Tuned necessary Airflow packages in GCP for seamless execution.

Scheduled and automated the entire process.

6. Data Visualization with Looker:
Connected BigQuery tables to Looker for interactive data exploration.

Designed custom dashboards to visualize employee trends, salary distribution, and department-wise analysis.

Automated Looker reports for business insights.

Technologies Used
Google Cloud Platform (GCP)

Cloud Composer (Apache Airflow)

Cloud Data Fusion

BigQuery

Cloud Storage

Looker (for data visualization)

Python (for data extraction and Airflow DAGs)

Wrangler (for data preprocessing)

SQL (for querying data in BigQuery)

Setup & Deployment
1. Clone the Repository

git clone https://github.com/your-repo-name.git
cd your-repo-name

2. Install Required Packages
Ensure you have the necessary dependencies installed:

pip install apache-airflow apache-airflow-providers-google google-cloud-storage pandas

3. Deploy Airflow DAG
Upload the dag.py file to your Airflow DAGs directory:

cp dags/your_dag.py $AIRFLOW_HOME/dags/
Restart Airflow to apply changes:

airflow scheduler & airflow webserver

4. Trigger the Pipeline
Trigger the Airflow DAG manually via the UI or using:
airflow dags trigger your_dag_name

5. Access Looker Dashboards
Connect Looker to BigQuery by selecting the correct dataset.

Build custom dashboards using LookML.

Share insights with stakeholders through scheduled reports.

Future Improvements
Implement real-time data streaming using Pub/Sub.

Add error handling & logging for better debugging.

Integrate machine learning models in BigQuery for predictive analytics.

Automate Looker reports with advanced scheduling and alerts.
